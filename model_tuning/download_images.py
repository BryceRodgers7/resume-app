"""
Automated Image Downloader for Training Dataset

Fully automated image downloading for training your classifier.
No manual downloading required - just run and go!

Methods supported:
1. Bing Image Search API (best quality, requires free Azure API key)
2. DuckDuckGo Search (no API key needed, fully automated)
3. Unsplash API (high quality, requires free API key)

Usage:
    # Bing (recommended if you have API key)
    python download_images.py --bing
    
    # DuckDuckGo (no API key needed!)
    python download_images.py --duckduckgo
    
    # Unsplash (highest quality)
    python download_images.py --unsplash
    
    # Auto-detect (tries available methods)
    python download_images.py --auto
"""

import requests
from pathlib import Path
import time
import json
from urllib.parse import quote, urlencode
import hashlib
from PIL import Image
import io
import os
import sys
import shutil
import random
from ddgs import DDGS

class ImageDownloader:
    def __init__(self, output_dir):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def download_image(self, url, filename):
        """Download a single image from URL"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            # Verify it's actually an image
            img = Image.open(io.BytesIO(response.content))
            
            # Convert to RGB if needed
            if img.mode in ('RGBA', 'LA', 'P'):
                img = img.convert('RGB')
            
            # Save image
            img.save(filename, 'JPEG', quality=95)
            return True
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Failed to download {url}: {str(e)[:50]}")
            return False
    
    def download_from_urls_file(self, urls_file, category):
        """Download images from a text file containing URLs"""
        urls_path = Path(urls_file)
        if not urls_path.exists():
            print(f"‚ùå File not found: {urls_file}")
            return 0
        
        with open(urls_path, 'r') as f:
            urls = [line.strip() for line in f if line.strip()]
        
        print(f"\nüì• Downloading {len(urls)} images for '{category}'...")
        
        downloaded = 0
        for i, url in enumerate(urls, 1):
            # Create filename from hash of URL
            url_hash = hashlib.md5(url.encode()).hexdigest()[:10]
            filename = self.output_dir / f"{category}_{i:04d}_{url_hash}.jpg"
            
            if filename.exists():
                print(f"  ‚è≠Ô∏è  Skipping (exists): {filename.name}")
                continue
            
            print(f"  [{i}/{len(urls)}] Downloading...", end='')
            if self.download_image(url, filename):
                downloaded += 1
                print(f" ‚úì Saved as {filename.name}")
            else:
                print(f" ‚úó Failed")
            
            # Be nice to servers
            time.sleep(0.5)
        
        return downloaded
    
    def search_duckduckgo(self, query, max_images=100):
        """
        Search DuckDuckGo for images (no API key needed!)
        Uses the ddgs library
        """
        print(f"\nüîç Searching DuckDuckGo for '{query}'...")
        
        all_urls = []
        
        try:
            # Use DDGS library for reliable image search
            ddgs = DDGS()
            results = ddgs.images(
                query=query,
                max_results=max_images
            )
            
            for result in results:
                if 'image' in result:
                    all_urls.append(result['image'])
                elif 'url' in result:
                    all_urls.append(result['url'])
            
            print(f"  Found {len(all_urls)} images for '{query}'")
                    
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Search error: {str(e)}")
            print("  üí° Tip: If DuckDuckGo doesn't work, try Bing API (free at Azure)")
        
        return all_urls[:max_images]
    
    # def search_bing(self, query, max_images=100, api_key=None):
    #     """
    #     Search Bing for images
    #     Requires Bing Search API key from Azure
    #     """
    #     if not api_key:
    #         print("\n‚ùå Bing API key required!")
    #         print("\nTo get a Bing Search API key:")
    #         print("1. Go to https://portal.azure.com")
    #         print("2. Create a 'Bing Search v7' resource (Free tier available)")
    #         print("3. Copy one of the API keys")
    #         print("4. Set BING_API_KEY environment variable or pass as parameter")
    #         return []
        
    #     print(f"\nüîç Searching Bing for '{query}'...")
        
    #     search_url = "https://api.bing.microsoft.com/v7.0/images/search"
    #     headers = {"Ocp-Apim-Subscription-Key": api_key}
        
    #     all_urls = []
    #     offset = 0
        
    #     while len(all_urls) < max_images:
    #         params = {
    #             "q": query,
    #             "count": min(150, max_images - len(all_urls)),
    #             "offset": offset,
    #             "imageType": "photo",
    #             "size": "medium"
    #         }
            
    #         try:
    #             response = requests.get(search_url, headers=headers, params=params, timeout=10)
    #             response.raise_for_status()
    #             data = response.json()
                
    #             images = data.get("value", [])
    #             if not images:
    #                 break
                
    #             for img in images:
    #                 all_urls.append(img["contentUrl"])
                
    #             print(f"  Found {len(images)} images (total: {len(all_urls)})")
                
    #             offset += len(images)
    #             time.sleep(1)  # Rate limiting
                
    #         except Exception as e:
    #             print(f"  ‚ö†Ô∏è  Search error: {str(e)}")
    #             break
        
    #     return all_urls[:max_images]
    
    # def search_unsplash(self, query, max_images=100, api_key=None):
    #     """
    #     Search Unsplash for high-quality images
    #     Requires free Unsplash API key from https://unsplash.com/developers
    #     """
    #     if not api_key:
    #         print("\n‚ùå Unsplash API key required!")
    #         print("\nTo get a free Unsplash API key:")
    #         print("1. Go to https://unsplash.com/developers")
    #         print("2. Register your application (free)")
    #         print("3. Copy the Access Key")
    #         print("4. Set UNSPLASH_API_KEY environment variable")
    #         return []
        
    #     print(f"\nüîç Searching Unsplash for '{query}'...")
        
    #     search_url = "https://api.unsplash.com/search/photos"
    #     headers = {"Authorization": f"Client-ID {api_key}"}
        
    #     all_urls = []
    #     page = 1
        
    #     while len(all_urls) < max_images:
    #         params = {
    #             "query": query,
    #             "per_page": 30,
    #             "page": page,
    #             "orientation": "landscape"
    #         }
            
    #         try:
    #             response = requests.get(search_url, headers=headers, params=params, timeout=10)
    #             response.raise_for_status()
    #             data = response.json()
                
    #             results = data.get("results", [])
    #             if not results:
    #                 break
                
    #             for result in results:
    #                 # Use regular size (not raw) for reasonable file sizes
    #                 all_urls.append(result["urls"]["regular"])
                
    #             print(f"  Found {len(results)} images (total: {len(all_urls)})")
                
    #             page += 1
    #             time.sleep(1)  # Rate limiting
                
    #         except Exception as e:
    #             print(f"  ‚ö†Ô∏è  Search error: {str(e)}")
    #             break
        
    #     return all_urls[:max_images]

def download_category_images(category, queries, method='duckduckgo', api_key=None, 
                             base_dir=None, train_pct=80, val_pct=20, images_per_query=100):
    """Download images for a single category using specified method"""
    
    print(f"\n{'='*70}")
    print(f"üì• Downloading {category.upper()} images")
    print(f"{'='*70}")
    
    # Create temporary download directory
    temp_dir = base_dir / 'temp' / category
    downloader = ImageDownloader(temp_dir)
    
    # Step 1: Collect URLs from ALL queries (don't stop early!)
    print(f"\nüîç Searching for {images_per_query} images per query ({len(queries)} queries)...")
    print(f"   Will split into {train_pct}% for training and {val_pct}% for validation")
    all_urls = []
    
    for i, query in enumerate(queries, 1):
        print(f"\n   Query {i}/{len(queries)}: '{query}'")
        if method == 'duckduckgo':
            urls = downloader.search_duckduckgo(query, images_per_query)
        # elif method == 'bing':
        #     urls = downloader.search_bing(query, images_per_query, api_key)
        # elif method == 'unsplash':
        #     urls = downloader.search_unsplash(query, images_per_query, api_key)
        else:
            print(f"‚ùå Unknown method: {method}")
            return 0
        
        all_urls.extend(urls)
        print(f"   ‚Üí Collected {len(urls)} URLs (total so far: {len(all_urls)})")
    
    print(f"\nüìä Total URLs collected: {len(all_urls)} from {len(queries)} queries")
    
    # Step 2: Download ALL images to temporary directory
    print(f"\nüíæ Downloading all {len(all_urls)} images...")
    downloaded_files = []
    
    for i, url in enumerate(all_urls, 1):
        filename = temp_dir / f"{category}_{i:04d}.jpg"
        
        if i % 50 == 0:  # Progress update every 50 images
            print(f"  Progress: {i}/{len(all_urls)} ({100*i/len(all_urls):.1f}%)")
        
        if downloader.download_image(url, filename):
            downloaded_files.append(filename)
        
        # Rate limiting
        time.sleep(0.5)
    
    print(f"\n‚úÖ Successfully downloaded {len(downloaded_files)} / {len(all_urls)} images for {category}")
    
    # Step 3: Sample and split into train and val sets
    print(f"\nüìä Splitting into {train_pct}% for training and {val_pct}% for validation")
    
    train_dir = base_dir / 'train' / category
    val_dir = base_dir / 'val' / category
    train_dir.mkdir(parents=True, exist_ok=True)
    val_dir.mkdir(parents=True, exist_ok=True)
    
    # Randomize order before splitting to avoid ordering bias
    random.shuffle(downloaded_files)
    
    # Split: first train_pct% images to train, rest to val
    train_files = downloaded_files[:int(len(downloaded_files) * train_pct / 100)]
    val_files = downloaded_files[int(len(downloaded_files) * train_pct / 100):]
    
    # Move first train_pct% of images to train set
    for i, src_file in enumerate(train_files, 1):
        dest_file = train_dir / f"{category}_train_{i:04d}.jpg"
        shutil.move(str(src_file), str(dest_file))
    
    # Move remaining images to val set
    for i, src_file in enumerate(val_files, 1):
        dest_file = val_dir / f"{category}_val_{i:04d}.jpg"
        shutil.move(str(src_file), str(dest_file))
    
    # Clean up temp directory
    if temp_dir.exists():
        shutil.rmtree(temp_dir)
    
    print(f"  ‚úì Moved {len(train_files)} images to train set ({train_pct}%)")
    print(f"  ‚úì Moved {len(val_files)} images to val set ({100-train_pct}%)")
    print(f"  ‚úì Total in dataset: {len(train_files) + len(val_files)} images")
    
    return len(train_files) + len(val_files)


def main():
    """Main function"""
    print("=" * 70)
    print("ü§ñ FULLY AUTOMATED Image Downloader")
    print("=" * 70)
    print("\n‚ú® No manual downloading required - just choose your method!\n")
    
    base_dir = Path(__file__).parent / 'dataset'
    
    # Define search queries for each category
    search_queries = {
        'bird': [
            'bird flying',
            'crow',
            'eagle',
            'parrot',
            'pidgeon',
        ],
        'plane': [
            'airplane flying',
            'airplane',
            'jet plane',
            'prop plane',
            'plane landing'
        ],
        'superman': [
            'superman flying',
            'superman',
            'superman costume',
            'superman comic',
            'superman live-action',
        ],
        'other': [
            'person',
            'crowd',
            'portrait',
            'face',
            'city',
            'street',
            'building',
            'skyscraper',
            'bridge',
            'interior',
            'room',
            'kitchen',
            'office',
            'furniture',
            'spaceship',
            'car',
            'truck',
            'train',
            'helicopter',
            'boat',
            'motorcycle',
            'bicycle',
            'construction',
            'pattern',
            'bulldozer',
            'tractor',
            'landscape',
            'mountain',
            'forest',
            'desert',
            'beach',
            'ocean',
            'waterfall',
            'flowers',
            'trees',
            'snow',
            'rocks',
            'underwater',
            'fish',
            'insects',
            'reptile',
            'dog',
            'cat',
            'food',
            'restaurant',
            'painting',
            'illustration',
            'anime',
            'cartoon',
            'screenshot',
            'texture',
        ]
    }
    
    # Check command line arguments
    # args = sys.argv[1:]
    
    # Determine method
    method = None
    # bing_api_key = os.environ.get('BING_API_KEY')
    # unsplash_api_key = os.environ.get('UNSPLASH_API_KEY')
    
    method = 'duckduckgo'
    api_key = None
    print("üéØ Using DuckDuckGo (no API key needed!)")
    print("üí° Tip: For better results, get a free Bing API key")
    
    print(f"\nüìÅ Dataset directory: {base_dir}")
    print("\n‚öôÔ∏è  Download Strategy:")
    print("   1. Search ALL queries and collect URLs")
    print("   2. Download ALL available images from all queries")
    print("   3. Randomly sample best images for train/val splits")
    print("\nüìä Expected Downloads:")
    print("   Bird/Plane/Superman (each):")
    print("     - Search: 100 images/query √ó 5 queries = ~500 images")
    print("     - Final dataset: 100 train + 25 val = 125 images")
    print("   Other:")
    print("     - Search: 10 images/query √ó 50 queries = ~500 images")
    print("     - Final dataset: 100 train + 25 val = 125 images")
    print("\n   üì• Total images to download: ~2000")
    print("   üíæ Final dataset size: 500 images (125 per category)")
    
    response = input("\n‚ñ∂Ô∏è  Start downloading? (y/n): ")
    if response.lower() != 'y':
        print("‚ùå Cancelled.")
        return
    
    print("\n" + "=" * 70)
    print("üöÄ Starting automated download...")
    print("=" * 70)
    print("\n‚è±Ô∏è  This will download ~2000 images (could take 30-60 minutes)...")
    print("‚òï Perfect time for a coffee break!\n")
    print("üí° Progress updates will show every 50 images downloaded\n")
    
    # Download images for each category
    start_time = time.time()
    total_downloaded = 0
    
    for category, queries in search_queries.items():
        # Set images_per_query based on category
        if category == 'other':
            images_per_query = 10  # 10 images per query for 'other'
        else:
            images_per_query = 100  # 100 images per query for bird/plane/superman
        
        downloaded = download_category_images(
            category=category,
            queries=queries,
            method=method,
            api_key=api_key,
            base_dir=base_dir,
            train_pct=80,
            val_pct=20,
            images_per_query=images_per_query
        )
        total_downloaded += downloaded
    
    elapsed = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("‚úÖ DOWNLOAD COMPLETE!")
    print("=" * 70)
    print(f"\nüìä Summary:")
    print(f"   - Time elapsed: {elapsed/60:.1f} minutes")
    print(f"   - Total images: {total_downloaded}")
    print(f"   - Location: {base_dir}")
    
    print("\nüéØ Next steps:")
    print("   1. Train the model:")
    print("      python train_classifier.py")
    print("\n   2. After training, analyze errors:")
    print("      python analyze_training_errors.py")
    print("\n   3. Review high-loss images and delete bad ones")
    print("\n   4. Retrain for better accuracy!")
    
    print("\nüí° Tip: Error analysis helps you identify and remove")
    print("   problematic images - this is the key to high accuracy!")
    print("\n" + "=" * 70)

if __name__ == '__main__':
    main()

